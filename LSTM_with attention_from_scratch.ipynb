{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "a3f3a15e-2cf6-443b-a77b-65f5eaaa3804"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating generator functions with training and evaluation data using trax\n",
    "train_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),\n",
    "                                 eval_holdout_size=0.01, # 1% for eval\n",
    "                                 train=True\n",
    "                                )\n",
    "\n",
    "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                data_dir='./data/',\n",
    "                                keys=('en', 'de'),\n",
    "                                eval_holdout_size=0.01, # 1% for eval                                \n",
    "                                train=False\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1BXy9emZvLnU"
   },
   "outputs": [],
   "source": [
    "train_stream = train_stream_fn()\n",
    "eval_stream = eval_stream_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "o92AZHRFrymz"
   },
   "outputs": [],
   "source": [
    "#Tokenizing by subwords\n",
    "data_dir = './data/'\n",
    "vocab_file = 'ende_32k.subword'\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=vocab_file, vocab_dir=data_dir)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=vocab_file, vocab_dir=data_dir)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "3xJOyfUTucVs"
   },
   "outputs": [],
   "source": [
    "#Adding <End of sentence> token to each tokenized sentence\n",
    "EOS =  1\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "\n",
    "# append EOS to the train data\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "\n",
    "# append EOS to the eval data\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "lJ2mp6fBR26h"
   },
   "outputs": [],
   "source": [
    "# Setup helper functions for tokenizing and detokenizing sentences\n",
    "\n",
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Encodes a string to an array of integers\n",
    "\n",
    "    Args:\n",
    "        input_str (str): human-readable string to encode\n",
    "        vocab_file (str): filename of the vocabulary text file\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "  \n",
    "    Returns:\n",
    "        numpy.ndarray: tokenized version of the input string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    \n",
    "    # Adding the batch dimension to the front of the shape\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Decodes an array of integers to a human readable string\n",
    "\n",
    "    Args:\n",
    "        integers (numpy.ndarray): array of integers to decode\n",
    "        vocab_file (str): filename of the vocabulary text file\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "  \n",
    "    Returns:\n",
    "        str: the decoded sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove the dimensions of size 1\n",
    "    integers = list(np.squeeze(integers))\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # Remove the EOS to decode only the original tokens\n",
    "    if EOS in integers:\n",
    "        integers = integers[:integers.index(EOS)] \n",
    "    \n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "K8s1TGBGzpwW"
   },
   "outputs": [],
   "source": [
    "#Truncating excesive lenght sentences\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "I0IglJuNyLJg"
   },
   "outputs": [],
   "source": [
    "#Bucketing by length with Trax\n",
    "boundaries =  [8,   16,  32, 64, 128, 256, 512] \n",
    "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
    "\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1] \n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]  \n",
    ")(filtered_eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "y932WN_e0RaK"
   },
   "outputs": [],
   "source": [
    "# Add masking for the padding .\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "nIPuYcTT2j8q"
   },
   "outputs": [],
   "source": [
    "#Setting encoder layers\n",
    "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    \"\"\" Input encoder runs on the input sentence and creates\n",
    "    activations that will be the keys and values for attention.\n",
    "    \n",
    "    Args:\n",
    "        input_vocab_size: int: vocab size of the input\n",
    "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
    "    Returns:\n",
    "        tl.Serial: The input encoder\n",
    "    \"\"\"\n",
    "    input_encoder = tl.Serial( \n",
    "        # create an embedding layer to convert tokens to vectors\n",
    "        tl.Embedding(vocab_size=input_vocab_size, d_feature= d_model),\n",
    "\n",
    "        # feed the embeddings to the LSTM layers. \n",
    "        [tl.LSTM(n_units = d_model) for l in range(n_encoder_layers)]\n",
    "    )\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "K0FKonBSxu8K"
   },
   "outputs": [],
   "source": [
    "#Setting pre attention decoder architecture\n",
    "#Note: Simple way to get the hidden states for the attention mechanism implementation later.\n",
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
    "    activations that are used as queries in attention.\n",
    "    \n",
    "    Args:\n",
    "        mode: str: 'train' or 'eval'\n",
    "        target_vocab_size: int: vocab size of the target\n",
    "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "    Returns:\n",
    "        tl.Serial: The pre-attention decoder\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a serial network\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        # shift right to insert start-of-sentence token and implement\n",
    "        # teacher forcing during training\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        # run an embedding layer to convert tokens to vectors\n",
    "        tl.Embedding(vocab_size=target_vocab_size,d_feature=d_model),\n",
    "        # feed to an LSTM layer\n",
    "        tl.LSTM(d_model)\n",
    "    ) \n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "UUSz27-qJ_Q5"
   },
   "outputs": [],
   "source": [
    "#Function to prepare attention layer input with queries,keys, values and attentionmask\n",
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
    "    \n",
    "    Args:\n",
    "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
    "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
    "        inputs fastnp.array(batch_size, padded_input_length): input tokens\n",
    "    \n",
    "    Returns:\n",
    "        queries, keys, values and mask for attention.\n",
    "    \"\"\"\n",
    "\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "    queries = decoder_activations\n",
    "\n",
    "    mask = inputs != 0\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "2r3huh_rT12n"
   },
   "outputs": [],
   "source": [
    "#Setting Attention layers\n",
    "def AttentionQKV(d_feature, n_heads=1, dropout=0.0, mode='train'):\n",
    "  \"\"\"Returns a layer that maps (q, k, v, mask) to (activations, mask).\n",
    "\n",
    "  See `Attention` above for further context/details.\n",
    "\n",
    "  Args:\n",
    "    d_feature: Depth/dimensionality of feature embedding.\n",
    "    n_heads: Number of attention heads.\n",
    "    dropout: Probababilistic rate for internal dropout applied to attention\n",
    "        activations (based on query-key pairs) before dotting them with values.\n",
    "    mode: Either 'train' or 'eval'.\n",
    "  \"\"\"\n",
    "  return cb.Serial(\n",
    "      cb.Parallel(\n",
    "          core.Dense(d_feature),\n",
    "          core.Dense(d_feature),\n",
    "          core.Dense(d_feature),\n",
    "      ),\n",
    "      PureAttention(  # pylint: disable=no-value-for-parameter\n",
    "          n_heads=n_heads, dropout=dropout, mode=mode),\n",
    "      core.Dense(d_feature),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "p6Fuua-JuHkZ"
   },
   "outputs": [],
   "source": [
    "#Build final model adding all modules created above\n",
    "def NMTAttn(input_vocab_size=33300,\n",
    "            target_vocab_size=33300,\n",
    "            d_model=1024,\n",
    "            n_encoder_layers=2,\n",
    "            n_decoder_layers=2,\n",
    "            n_attention_heads=4,\n",
    "            attention_dropout=0.0,\n",
    "            mode='train'):\n",
    "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
    "\n",
    "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
    "    an English sentence (tokenized) and its translation into German (tokenized).\n",
    "\n",
    "    Args:\n",
    "    input_vocab_size: int: vocab size of the input\n",
    "    target_vocab_size: int: vocab size of the target\n",
    "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
    "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
    "    n_attention_heads: int: number of attention heads\n",
    "    attention_dropout: float, dropout for the attention layer\n",
    "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
    "\n",
    "    Returns:\n",
    "    An LSTM sequence-to-sequence model with attention.\n",
    "    \"\"\"\n",
    "\n",
    "    input_encoder = input_encoder_fn(input_vocab_size,d_model,n_encoder_layers)\n",
    "    pre_attention_decoder = pre_attention_decoder_fn(mode,target_vocab_size,d_model)\n",
    "\n",
    "    # create a serial network\n",
    "    model = tl.Serial( \n",
    "        \n",
    "    # copy input tokens and target tokens as they will be needed later.\n",
    "    tl.Select([0,1,0,1]),\n",
    "      \n",
    "    # run input encoder on the input and pre-attention decoder the target.\n",
    "    tl.Parallel(input_encoder, pre_attention_decoder),\n",
    "      \n",
    "    # prepare queries, keys, values and mask for attention.\n",
    "    tl.Fn('PrepareAttentionInput', f=prepare_attention_input, n_out=4),\n",
    "      \n",
    "    # run the AttentionQKV layer\n",
    "    # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
    "    tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "    \n",
    "    # drop attention mask (i.e. index = None\n",
    "    tl.Select([0,2]),\n",
    "      \n",
    "    # run the rest of the RNN decoder\n",
    "    [tl.LSTM(n_units=d_model) for _ in range(n_decoder_layers)],\n",
    "      \n",
    "    # prepare output by making it the right size\n",
    "    tl.Dense(n_units=target_vocab_size),\n",
    "      \n",
    "    # Transforming output from dense with Log-softmax \n",
    "    tl.LogSoftmax()\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yB2QXyVj126d",
    "outputId": "8bc5d193-d9a0-440b-8c07-8a434778c6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial_in2_out2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Parallel_in2_out2[\n",
      "    Serial[\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "    Serial[\n",
      "      Serial[\n",
      "        ShiftRight(1)\n",
      "      ]\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "  ]\n",
      "  PrepareAttentionInput_in3_out4\n",
      "  Serial_in4_out2[\n",
      "    Branch_in4_out3[\n",
      "      None\n",
      "      Serial_in4_out2[\n",
      "        _in4_out4\n",
      "        Serial_in4_out2[\n",
      "          Parallel_in3_out3[\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "          ]\n",
      "          PureAttention_in4_out2\n",
      "          Dense_1024\n",
      "        ]\n",
      "        _in2_out2\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Select[0,2]_in3_out2\n",
      "  LSTM_1024\n",
      "  LSTM_1024\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = NMTAttn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "3PV25qBc3Pb5"
   },
   "outputs": [],
   "source": [
    "#Setting training generator  with the parameters below\n",
    "training_generator = training.TrainTask(\n",
    "        labeled_data= train_batch_stream,\n",
    "        loss_layer= tl.CrossEntropyLoss(), #loss function to optimize\n",
    "        optimizer= trax.optimizers.Adam(0.01), #optimizer\n",
    "        lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01), #learning rate schedule which follows sqrt decay\n",
    "        n_steps_per_checkpoint= 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "YTaSOQSs7Xgm"
   },
   "outputs": [],
   "source": [
    "#Setting evaluation generator\n",
    "eval_generator = training.EvalTask(\n",
    "    labeled_data=eval_batch_stream,\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVdPBzRy85GN",
    "outputId": "027d90bb-f9fb-42b2-8c74-ad6b3ef069a4"
   },
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = 'output_dir/'\n",
    "\n",
    "# remove old model if it exists. restarts training.\n",
    "!rm -f ~/output_dir/model.pkl.gz  \n",
    "\n",
    "# define the training loop\n",
    "training_loop = training.Loop(NMTAttn(mode='train'),\n",
    "                              training_generator,\n",
    "                              eval_tasks=[eval_generator],\n",
    "                              output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dPU8kHj9SQ6",
    "outputId": "f0a4ea7b-b43c-4df7-eb54-32bebd2bb40d"
   },
   "outputs": [],
   "source": [
    "training_loop.run(10) #10 epochs training test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtRcCWAF7vLO"
   },
   "source": [
    "# Loading checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "btCl6UNq7n08"
   },
   "outputs": [],
   "source": [
    "# instantiate the model built in eval mode\n",
    "model = NMTAttn(mode='eval')\n",
    "\n",
    "# initialize weights from a pre-trained model\n",
    "model.init_from_file(\"model.pkl.gz\", weights_only=True)\n",
    "model = tl.Accelerate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my_mJ4oL4X8e"
   },
   "source": [
    "# Sampling and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "EBs_JeXpCLmt"
   },
   "outputs": [],
   "source": [
    "#Inference function for next symbol prediction using model previously trained\n",
    "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
    "    \"\"\"Returns the index of the next token.\n",
    "\n",
    "    Args:\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
    "        cur_output_tokens (list): tokenized representation of previously translated words\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        int: index of the next token in the translated sentence\n",
    "        float: log probability of the next symbol\n",
    "    \"\"\"\n",
    "    # set the length of the current output tokens\n",
    "    token_length = len(cur_output_tokens)\n",
    "    \n",
    "    # calculate next power of 2 for padding length \n",
    "    padded_length = np.power(2,int(np.ceil(np.log2(token_length +1))))\n",
    "\n",
    "    # pad cur_output_tokens up to the padded_length\n",
    "    padded = cur_output_tokens + list(np.zeros(padded_length-token_length,dtype=int))\n",
    "    \n",
    "    # model expects the output to have an axis for the batch size in front so\n",
    "    # convert `padded` list to a numpy array with shape (1, <padded_length>)\n",
    "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
    "\n",
    "    # get the model prediction\n",
    "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
    "    \n",
    "    # get log probabilities from the last token output\n",
    "    log_probs = output[0,token_length,:]\n",
    "\n",
    "    # get the next symbol by getting a logsoftmax sample \n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature)) \n",
    "    return symbol, float(log_probs[symbol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "9hOPu952-xmv"
   },
   "outputs": [],
   "source": [
    "# Calling function next symbol until EOS token is selected. Temperature will condition the randomness of the sampling, selecting randomly one of the possible tokens depeding on their log probs in the probability distribution.\n",
    "# Temperature = 0 means no random sampling and greedy decoding will occur(the token with highest log prob will be selected as next symbol)\n",
    "# With high temperatures tokens with low log probs will have higher probabilities of being selected in the random sampling\n",
    "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "    \"\"\"Returns the translated sentence.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (str): sentence to translate.\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list, str, float)\n",
    "            list of int: tokenized version of the translated sentence\n",
    "            float: log probability of the translated sentence\n",
    "            str: the translated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # encode the input sentence\n",
    "    input_tokens = tokenize(input_sentence,vocab_file,vocab_dir)\n",
    "    \n",
    "    cur_output_tokens = []\n",
    "    cur_output = 0\n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # check that the current output is not the end of sentence token\n",
    "    while cur_output != EOS:\n",
    "        \n",
    "        # update the current output token by getting the index of the next word \n",
    "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
    "        \n",
    "        # append the current output token to the list of output tokens\n",
    "        cur_output_tokens.append(cur_output)  \n",
    "    \n",
    "    # detokenize the output tokens\n",
    "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n",
    "\n",
    "    return cur_output_tokens, log_prob, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "9s_VsHp4BrQe"
   },
   "outputs": [],
   "source": [
    "# Minimum Bayes Risk decoding will be carried.\n",
    "# First  a set of random samples will be generated\n",
    "# Then, they will be scored with ROUGE score with each other\n",
    "# Finally the best candidate through MBR decoding will be the one with highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "LlCLm-2jCwNW"
   },
   "outputs": [],
   "source": [
    "# Funtion to generate several random samples\n",
    "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "    \"\"\"Generates samples using sampling_decode()\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to translate.\n",
    "        n_samples (int): number of samples to generate\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list, list)\n",
    "            list of lists: token list per sample\n",
    "            list of floats: log probability per sample\n",
    "    \"\"\"\n",
    "    # define lists to contain samples and probabilities\n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    # run a for loop to generate n samples\n",
    "    for _ in range(n_samples):\n",
    "        \n",
    "        # get a sample using the sampling_decode() function\n",
    "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir, next_symbol=next_symbol)\n",
    "        \n",
    "        # append the token list to the samples list\n",
    "        samples.append(sample)\n",
    "        \n",
    "        # append the log probability to the log_probs list\n",
    "        log_probs.append(logp)\n",
    "                \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "rhDAMUPmC4qc"
   },
   "outputs": [],
   "source": [
    "#Function to calculate rouge score between pairs.\n",
    "def rouge1_similarity(system, reference):\n",
    "    \"\"\"Returns the ROUGE-1 score between two token lists\n",
    "\n",
    "    Args:\n",
    "        system (list of int): tokenized version of the system translation\n",
    "        reference (list of int): tokenized version of the reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: overlap between the two token lists\n",
    "    \"\"\"    \n",
    "\n",
    "    # make a frequency table of the system tokens \n",
    "    sys_counter = Counter(system)\n",
    "    \n",
    "    # make a frequency table of the reference tokens \n",
    "    ref_counter = Counter(reference)\n",
    "    \n",
    "    # initialize overlap to 0\n",
    "    overlap = 0\n",
    "    \n",
    "    # run a for loop over the sys_counter object \n",
    "    for token in sys_counter:\n",
    "        \n",
    "        # lookup the value of the token in the sys_counter dictionary \n",
    "        token_count_sys = sys_counter.get(token,0)\n",
    "        \n",
    "        # lookup the value of the token in the ref_counter dictionary \n",
    "        token_count_ref = ref_counter.get(token,0)\n",
    "        \n",
    "        # update the overlap by getting the smaller number between the two token counts above\n",
    "        overlap += min(token_count_sys, token_count_ref)\n",
    "    \n",
    "    # get the precision\n",
    "    precision = overlap/len(system)\n",
    "    \n",
    "    # get the recall\n",
    "    recall = overlap/len(reference)\n",
    "    \n",
    "    if precision + recall != 0: \n",
    "        # compute the f1-score\n",
    "        rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
    "    else:\n",
    "        rouge1_score = 0 \n",
    "    \n",
    "    return rouge1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "JMNFLl4QJ-1q"
   },
   "outputs": [],
   "source": [
    "#Calculates ROUGE score arithmetic mean for each sample.\n",
    "def average_overlap(similarity_fn, samples, *ignore_params):\n",
    "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        similarity_fn (function): similarity function used to compute the overlap\n",
    "        samples (list of lists): tokenized version of the translated sentences\n",
    "        *ignore_params: additional parameters will be ignored\n",
    "\n",
    "    Returns:\n",
    "        dict: scores of each sample\n",
    "            key: index of the sample\n",
    "            value: score of the sample\n",
    "    \"\"\"  \n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "       \n",
    "        # initialize overlap\n",
    "        overlap = 0\n",
    "        \n",
    "        # run a for loop for each sample\n",
    "        for index_sample, sample in enumerate(samples): \n",
    "\n",
    "            # skip if the candidate index is the same as the sample index\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # get the overlap between candidate and sample using the similarity function\n",
    "            sample_overlap = rouge1_similarity(candidate, sample)\n",
    "            \n",
    "            # add the sample overlap to the total overlap\n",
    "            overlap += sample_overlap\n",
    "            \n",
    "        # get the score for the candidate by computing the average\n",
    "        score = overlap/(len(samples)-1)\n",
    "        \n",
    "        # save the score in the dictionary.\n",
    "        scores[index_candidate] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "_04pQyJWJCG1"
   },
   "outputs": [],
   "source": [
    "#Funtion to implement minimum bayes risk decoding using previous helper functions\n",
    "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, generate_samples=generate_samples, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to translate.\n",
    "        n_samples (int): number of samples to generate\n",
    "        score_fn (function): function that generates the score for each sample\n",
    "        similarity_fn (function): function used to compute the overlap between a pair of samples\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate samples\n",
    "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn=NMTAttn, temperature=temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "    \n",
    "    # use the scoring function to get a dictionary of scores\n",
    "    scores = score_fn(similarity_fn, samples, log_probs)\n",
    "    \n",
    "    # find the key with the highest score\n",
    "    max_score_key = max(scores, key=scores.get)\n",
    "    \n",
    "    # detokenize the token list associated with the max_score_key\n",
    "    translated_sentence = detokenize(samples[max_score_key], vocab_file, vocab_dir)\n",
    "    \n",
    "    return (translated_sentence, max_score_key, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoS55BPMP-6o"
   },
   "source": [
    "# Inference with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "Ity8XxfoLQTm"
   },
   "outputs": [],
   "source": [
    "examples = ['Robert was a good king.', \n",
    "            'He had a great army.',\n",
    "            'He wanted to bring peace to his kingdom.',\n",
    "            'There were many others who wanted to become king.',\n",
    "            'They started plotting against him.',\n",
    "            'Their plots were failing because of some trusted friends of the king.',\n",
    "            'Then they started killing those trusted friends.',\n",
    "            'Eventually, they succeeded in their plan of killing the king.',\n",
    "            'Did they make a good move?',\n",
    "            'Can they find a new king without dispute?',\n",
    "            'After the death of the king, everyone wanted to be a king.',\n",
    "            'A great chaos broke out in the kingdom.',\n",
    "            'People were anxious and unhappy.',\n",
    "            'War does not bring anything good to the common people.'\n",
    "            'It only brings sorrow and disma']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "a0jh0K-ZKVbc",
    "outputId": "3b6913cc-e481-44c6-e14e-b87c7008385a"
   },
   "outputs": [],
   "source": [
    "#Greedy decoding over examples(temperature = 0)\n",
    "greedy_decoded_sentences=[]\n",
    "for example in examples:\n",
    "    translation = mbr_decode(example, 4, average_overlap, rouge1_similarity, model, temperature= 0, vocab_file=vocab_file, vocab_dir=data_dir)[0]\n",
    "    greedy_decoded_sentences.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert war ein guter König.\n",
      "Er hatte eine große Armee.\n",
      "Er wollte seinem Reich Frieden bringen.\n",
      "Es gab viele andere, die sich zum König machen wollten.\n",
      "Sie begannen, gegen ihn zu protestieren.\n",
      "Ihre Grundstücke sind durch einige treuhändige Freunde des Königs nicht zu übertreffen.\n",
      "Dann begannen sie, diese vertrauenswürdigen Freunde zu töten.\n",
      "Schließlich gelang es ihnen, den König zu töten.\n",
      "Haben sie einen guten Schritt gemacht?\n",
      "Können sie einen neuen König finden ohne Streit?\n",
      "Nach dem Tod des Königs wünschte jeder König.\n",
      "Im Reich ist ein großer Chaos ausbrachen.\n",
      "Die Leute waren beunruhigt und unglücklich.\n",
      "Krieg bringt nichts Gutes für das Volk.Es bringt nur Trauer und Elend.\n"
     ]
    }
   ],
   "source": [
    "for sentence in greedy_decoded_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "p3wbkVeOLGQK",
    "outputId": "ad665152-ec2a-4d53-f3c6-5af520b7f7c1"
   },
   "outputs": [],
   "source": [
    "#Random sampling with temperature = 0.6 over examples\n",
    "t6_sentences=[]\n",
    "for example in examples:\n",
    "    translation = mbr_decode(example, 4, average_overlap, rouge1_similarity, model, temperature= 0.6, vocab_file=vocab_file, vocab_dir=data_dir)[0]\n",
    "    t6_sentences.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "7jGlqOUjPeS3"
   },
   "outputs": [],
   "source": [
    "Translator_table = pd.DataFrame({'Examples':examples,'Translated sentences with Temperature=0':greedy_decoded_sentences,'Translated sentences with temperature 0.6':t6_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Examples</th>\n",
       "      <th>Translated sentences with Temperature=0</th>\n",
       "      <th>Translated sentences with temperature 0.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert was a good king.</td>\n",
       "      <td>Robert war ein guter König.</td>\n",
       "      <td>Robert war ein guter König.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He had a great army.</td>\n",
       "      <td>Er hatte eine große Armee.</td>\n",
       "      <td>Er hatte eine große Armee.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He wanted to bring peace to his kingdom.</td>\n",
       "      <td>Er wollte seinem Reich Frieden bringen.</td>\n",
       "      <td>Er wollte seinem Reich Frieden bringen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There were many others who wanted to become king.</td>\n",
       "      <td>Es gab viele andere, die sich zum König machen...</td>\n",
       "      <td>Es gab viele andere, die sich zum König machen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They started plotting against him.</td>\n",
       "      <td>Sie begannen, gegen ihn zu protestieren.</td>\n",
       "      <td>Sie begannen, gegen ihn zu klagen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Their plots were failing because of some trust...</td>\n",
       "      <td>Ihre Grundstücke sind durch einige treuhändige...</td>\n",
       "      <td>Ihre Grundstücke sind aufgrund gewisser Treuhä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Then they started killing those trusted friends.</td>\n",
       "      <td>Dann begannen sie, diese vertrauenswürdigen Fr...</td>\n",
       "      <td>Dann begannen sie, diese vertrauenswürdigen Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eventually, they succeeded in their plan of ki...</td>\n",
       "      <td>Schließlich gelang es ihnen, den König zu töten.</td>\n",
       "      <td>Schließlich gelang es ihnen ihren Plan, den Kö...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Did they make a good move?</td>\n",
       "      <td>Haben sie einen guten Schritt gemacht?</td>\n",
       "      <td>Haben sie einen guten Schritt gemacht?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Can they find a new king without dispute?</td>\n",
       "      <td>Können sie einen neuen König finden ohne Streit?</td>\n",
       "      <td>Können sie einen neuen König finden, ohne Streit?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>After the death of the king, everyone wanted t...</td>\n",
       "      <td>Nach dem Tod des Königs wünschte jeder König.</td>\n",
       "      <td>Nach dem Tod des Königs wollte jeder König sein.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A great chaos broke out in the kingdom.</td>\n",
       "      <td>Im Reich ist ein großer Chaos ausbrachen.</td>\n",
       "      <td>Im Reich wurde ein großer Chaos ausgebrochen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>People were anxious and unhappy.</td>\n",
       "      <td>Die Leute waren beunruhigt und unglücklich.</td>\n",
       "      <td>Die Menschen waren besorgt und unzufrieden.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>War does not bring anything good to the common...</td>\n",
       "      <td>Krieg bringt nichts Gutes für das Volk.Es brin...</td>\n",
       "      <td>Krieg bringt dem Volk nichts Gutes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Examples  \\\n",
       "0                             Robert was a good king.   \n",
       "1                                He had a great army.   \n",
       "2            He wanted to bring peace to his kingdom.   \n",
       "3   There were many others who wanted to become king.   \n",
       "4                  They started plotting against him.   \n",
       "5   Their plots were failing because of some trust...   \n",
       "6    Then they started killing those trusted friends.   \n",
       "7   Eventually, they succeeded in their plan of ki...   \n",
       "8                          Did they make a good move?   \n",
       "9           Can they find a new king without dispute?   \n",
       "10  After the death of the king, everyone wanted t...   \n",
       "11            A great chaos broke out in the kingdom.   \n",
       "12                   People were anxious and unhappy.   \n",
       "13  War does not bring anything good to the common...   \n",
       "\n",
       "              Translated sentences with Temperature=0  \\\n",
       "0                         Robert war ein guter König.   \n",
       "1                          Er hatte eine große Armee.   \n",
       "2             Er wollte seinem Reich Frieden bringen.   \n",
       "3   Es gab viele andere, die sich zum König machen...   \n",
       "4            Sie begannen, gegen ihn zu protestieren.   \n",
       "5   Ihre Grundstücke sind durch einige treuhändige...   \n",
       "6   Dann begannen sie, diese vertrauenswürdigen Fr...   \n",
       "7    Schließlich gelang es ihnen, den König zu töten.   \n",
       "8              Haben sie einen guten Schritt gemacht?   \n",
       "9    Können sie einen neuen König finden ohne Streit?   \n",
       "10      Nach dem Tod des Königs wünschte jeder König.   \n",
       "11          Im Reich ist ein großer Chaos ausbrachen.   \n",
       "12        Die Leute waren beunruhigt und unglücklich.   \n",
       "13  Krieg bringt nichts Gutes für das Volk.Es brin...   \n",
       "\n",
       "            Translated sentences with temperature 0.6  \n",
       "0                         Robert war ein guter König.  \n",
       "1                          Er hatte eine große Armee.  \n",
       "2             Er wollte seinem Reich Frieden bringen.  \n",
       "3   Es gab viele andere, die sich zum König machen...  \n",
       "4                  Sie begannen, gegen ihn zu klagen.  \n",
       "5   Ihre Grundstücke sind aufgrund gewisser Treuhä...  \n",
       "6   Dann begannen sie, diese vertrauenswürdigen Fr...  \n",
       "7   Schließlich gelang es ihnen ihren Plan, den Kö...  \n",
       "8              Haben sie einen guten Schritt gemacht?  \n",
       "9   Können sie einen neuen König finden, ohne Streit?  \n",
       "10   Nach dem Tod des Königs wollte jeder König sein.  \n",
       "11      Im Reich wurde ein großer Chaos ausgebrochen.  \n",
       "12        Die Menschen waren besorgt und unzufrieden.  \n",
       "13                Krieg bringt dem Volk nichts Gutes.  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Translator_table"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
